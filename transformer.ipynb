{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "*28.12_CALISIYO*",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMPxg/MRzNWg2jNADz9T3mO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nurkbts/yenidepom/blob/master/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KqK-M0VziiO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0ccffa4-7494-42bb-c343-7ad2da6ffe32"
      },
      "source": [
        "from numpy import vstack,argmax\r\n",
        "from pandas import read_csv\r\n",
        "import torch\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset,DataLoader,random_split\r\n",
        "from torch.nn import *\r\n",
        "import pandas as pd \r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from loss import OZELoss\r\n",
        "from transformer import Transformer\r\n",
        "import seaborn as sns\r\n",
        "from tqdm import tqdm\r\n",
        "import datetime\r\n",
        "from utils_ import compute_loss\r\n",
        "#from plot_functions import map_plot_function, plot_values_distribution, plot_error_distribution, plot_errors_threshold, plot_visual_sampl\r\n",
        "class CSVDataset(Dataset):\r\n",
        "    def __init__(self, path):\r\n",
        "        KINEMATICS_USECOLS = [c-1 for c in [39, 40, 41, 51, 52, 53, 57, 58, 59, 60, 70, 71, 72, 76,77]]\r\n",
        "        df = read_csv(path,sep=',',usecols=KINEMATICS_USECOLS)\r\n",
        "        self.X, self.y = df.values[:, :-1], df.values[:, -1]\r\n",
        "        self.X, self.y = self.X.astype('float32'), LabelEncoder().fit_transform(self.y)\r\n",
        "        #veri normalizasyonu\r\n",
        "        mu =  self.X.mean(axis=0, keepdims=True)\r\n",
        "        sigma =  self.X.std(axis=0, keepdims=True)\r\n",
        "        self.X = ( self.X - mu) / sigma \r\n",
        "        self.X= torch.Tensor(self.X)\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.X)\r\n",
        "    def __getitem__(self, idx):\r\n",
        "    #  if torch.is_tensor(idx):\r\n",
        "     #  idx = idx.tolist()\r\n",
        "      return [self.X[idx], self.y[idx]]\r\n",
        "def prepare_data(path):\r\n",
        "    dataset = CSVDataset(path)\r\n",
        "    train_dl  = DataLoader(dataset, batch_size=BATCH_SIZE,shuffle=True, num_workers=NUM_WORKERS,pin_memory=False)\r\n",
        "    return train_dl\r\n",
        "BATCH_SIZE =8\r\n",
        "NUM_WORKERS= 0\r\n",
        "LR =0.001\r\n",
        "EPOCHS=20\r\n",
        "d_model =64\r\n",
        "q=8 \r\n",
        "v=8\r\n",
        "h = 8 \r\n",
        "N = 4\r\n",
        "attention_size = 12\r\n",
        "dropout = 0.2\r\n",
        "pe = None\r\n",
        "chunk_mode = None \r\n",
        "d_input = 14\r\n",
        "d_output= 10\r\n",
        "path = 'cdefg.txt'\r\n",
        "train_dl = prepare_data(path)\r\n",
        "dataset_test =CSVDataset('b.txt')\r\n",
        "test_dl = DataLoader(dataset_test, batch_size=BATCH_SIZE,shuffle=False, num_workers=NUM_WORKERS)\r\n",
        "print(len(train_dl.dataset),  len(test_dl.dataset))\r\n",
        "sns.set()\r\n",
        "if torch.cuda.is_available():\r\n",
        "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \r\n",
        "    print(\"Running on the GPU\")\r\n",
        "\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(f\"Using device {device}\")\r\n",
        "\r\n",
        "net = Transformer(d_input, d_model, d_output, q, v, h, N, attention_size=attention_size, dropout=dropout, chunk_mode=chunk_mode, pe=pe)\r\n",
        "#optimizer = optim.Adam(net.parameters(), lr=LR)\r\n",
        "optimizer = optim.SGD(net.parameters(), lr=LR,weight_decay=0.5, momentum=0.9)\r\n",
        "#optimizer = optim.SGD(net.parameters(), lr=LR, momentum=0.9)\r\n",
        "loss_function = nn.CrossEntropyLoss() \r\n",
        "model_save_path = f'models/model_{datetime.datetime.now().strftime(\"%Y_%m_%d__%H%M%S\")}.pth'\r\n",
        "for idx_epoch in range(EPOCHS):\r\n",
        "    running_loss = 0\r\n",
        "    with tqdm(total=len(train_dl.dataset), desc=f\"[Epoch {idx_epoch+1:3d}/{EPOCHS}]\") as pbar:\r\n",
        "      for idx_batch, (x, y) in enumerate(train_dl):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        netout = net(x)\r\n",
        "        loss = loss_function(netout, y)\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        running_loss += loss.item()\r\n",
        "        pbar.set_postfix({'loss': running_loss/(idx_batch+1)})\r\n",
        "        pbar.update(x.shape[0])\r\n",
        "\r\n",
        "# evaluate the model\r\n",
        "def evaluate_model(test_dl, model):\r\n",
        "    predictions, actuals = list(), list()\r\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\r\n",
        "        yhat = model(inputs)\r\n",
        "        yhat = yhat.detach().numpy()\r\n",
        "        actual = targets.numpy()\r\n",
        "        yhat = argmax(yhat, axis=1)\r\n",
        "        actual = actual.reshape((len(actual), 1))\r\n",
        "        yhat = yhat.reshape((len(yhat), 1))\r\n",
        "        predictions.append(yhat)\r\n",
        "        actuals.append(actual)\r\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\r\n",
        "    acc = accuracy_score(actuals, predictions)\r\n",
        "    return acc\r\n",
        "\r\n",
        "acc = evaluate_model(test_dl, net)\r\n",
        "print('Accuracy: %.3f' % acc)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r[Epoch   1/20]:   0%|          | 0/74725 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "74725 18354\n",
            "Using device cpu\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch   1/20]:  70%|███████   | 52656/74725 [08:18<02:56, 125.16it/s, loss=2.3]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "2Z4o3J2txhlx",
        "outputId": "057ee950-47fa-4a1a-e6b0-078332cb154d"
      },
      "source": [
        "from numpy import vstack,argmax\r\n",
        "from pandas import read_csv\r\n",
        "import torch\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset,DataLoader,random_split\r\n",
        "from torch.nn import *\r\n",
        "import pandas as pd \r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from loss import OZELoss\r\n",
        "from transformer import Transformer\r\n",
        "import seaborn as sns\r\n",
        "from tqdm import tqdm\r\n",
        "import datetime\r\n",
        "from utils_ import compute_loss\r\n",
        "#from plot_functions import map_plot_function, plot_values_distribution, plot_error_distribution, plot_errors_threshold, plot_visual_sampl\r\n",
        "class CSVDataset(Dataset):\r\n",
        "    def __init__(self, path):\r\n",
        "        KINEMATICS_USECOLS = [c-1 for c in [39, 40, 41, 51, 52, 53, 57, 58, 59, 60, 70, 71, 72, 76]]\r\n",
        "        train = read_csv('x.txt',sep=',',usecols=KINEMATICS_USECOLS)\r\n",
        "        test = read_csv('_y.txt')\r\n",
        "        self.X, self.y = df.values[:, :-1], df.values[:, -1]\r\n",
        "        self.X, self.y = train.values, test.values\r\n",
        "        print(self.X.shape)\r\n",
        "        self.X = self.X.astype('float32')\r\n",
        "        self.y =  LabelEncoder().fit_transform(self.y)\r\n",
        "        self.X= torch.Tensor(self.X)\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.X)\r\n",
        "    def __getitem__(self, idx):\r\n",
        "    #  if torch.is_tensor(idx):\r\n",
        "     #  idx = idx.tolist()\r\n",
        "      return [self.X[idx], self.y[idx]]\r\n",
        "    def get_splits(self, n_test=0.33):\r\n",
        "        test_size = round(n_test * len(self.X))\r\n",
        "        val_size =1000\r\n",
        "        train_size = len(self.X) - test_size-1000\r\n",
        "        return random_split(self, [train_size, val_size, test_size])\r\n",
        "def prepare_data(path):\r\n",
        "    dataset = CSVDataset(path)\r\n",
        "    train, val, test = dataset.get_splits()\r\n",
        "    train_dl ,val_dl,test_dl = DataLoader(train, batch_size=BATCH_SIZE,shuffle=True, num_workers=NUM_WORKERS,pin_memory=False),DataLoader(val, batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_WORKERS),DataLoader(test, batch_size=BATCH_SIZE,shuffle=False, num_workers=NUM_WORKERS)\r\n",
        "    return train_dl, val_dl, test_dl\r\n",
        "BATCH_SIZE ,NUM_WORKERS, LR,EPOCHS,d_model ,q ,v, h, N,attention_size,dropout,pe,chunk_mode,d_input,d_output= 8,0,2e-4,20,64,8 ,8,8,4,16 ,0.2,None,None,14,10\r\n",
        "path = 'merge_from_ofoct (1).txt'\r\n",
        "train_dl, val_dl, test_dl = prepare_data(path)\r\n",
        "print(len(train_dl.dataset), len(val_dl.dataset),  len(test_dl.dataset))\r\n",
        "sns.set()\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(f\"Using device {device}\")\r\n",
        "net = Transformer(d_input, d_model, d_output, q, v, h, N, attention_size=attention_size, dropout=dropout, chunk_mode=chunk_mode, pe=pe).to(device)\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=LR)\r\n",
        "loss_function = nn.CrossEntropyLoss() \r\n",
        "#loss_function = OZELoss(alpha=0.3)\r\n",
        "model_save_path = f'models/model_{datetime.datetime.now().strftime(\"%Y_%m_%d__%H%M%S\")}.pth'\r\n",
        "for idx_epoch in range(EPOCHS):\r\n",
        "    running_loss = 0\r\n",
        "    with tqdm(total=len(train_dl.dataset), desc=f\"[Epoch {idx_epoch+1:3d}/{EPOCHS}]\") as pbar:\r\n",
        "      for idx_batch, (x, y) in enumerate(train_dl):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        #print(idx_batch)\r\n",
        "        netout = net(x.to(device))\r\n",
        "        #print(netout.size())\r\n",
        "        #print(\"SSSSSSSS\")\r\n",
        "        #print(y.size())\r\n",
        "        loss = loss_function(netout, y)\r\n",
        "        #print(\"eee\")\r\n",
        "        loss.backward()\r\n",
        "        #print(\"kkkk\")\r\n",
        "        optimizer.step()\r\n",
        "        running_loss += loss.item()\r\n",
        "        pbar.set_postfix({'loss': running_loss/(idx_batch+1)})\r\n",
        "        pbar.update(x.shape[0])\r\n",
        "\r\n",
        "# evaluate the model\r\n",
        "def evaluate_model(test_dl, model):\r\n",
        "    predictions, actuals = list(), list()\r\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\r\n",
        "        # evaluate the model on the test set\r\n",
        "        yhat = model(inputs)\r\n",
        "        # retrieve numpy array\r\n",
        "        yhat = yhat.detach().numpy()\r\n",
        "        actual = targets.numpy()\r\n",
        "        # convert to class labels\r\n",
        "        yhat = argmax(yhat, axis=1)\r\n",
        "        # reshape for stacking\r\n",
        "        actual = actual.reshape((len(actual), 1))\r\n",
        "        yhat = yhat.reshape((len(yhat), 1))\r\n",
        "        # store\r\n",
        "        predictions.append(yhat)\r\n",
        "        actuals.append(actual)\r\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\r\n",
        "    # calculate accuracy\r\n",
        "    acc = accuracy_score(actuals, predictions)\r\n",
        "    return acc\r\n",
        "\r\n",
        "acc = evaluate_model(test_dl, net)\r\n",
        "print('Accuracy: %.3f' % acc)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c98b630766d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mNUM_WORKERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_model\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mq\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mchunk_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_output\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'merge_from_ofoct (1).txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c98b630766d5>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrandom_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCSVDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtrain_dl\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_WORKERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_WORKERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_WORKERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c98b630766d5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKINEMATICS_USECOLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_y.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "DYNODW_ePf8f",
        "outputId": "5cc604ce-7301-42ef-ef05-eac84fa2b8d0"
      },
      "source": [
        "#VAL OLMADAN\r\n",
        "from numpy import vstack,argmax\r\n",
        "from pandas import read_csv\r\n",
        "import torch\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from torch import Tensor\r\n",
        "from torch.utils.data import Dataset,DataLoader,random_split\r\n",
        "from torch.nn import *\r\n",
        "import pandas as pd \r\n",
        "import torch.nn as nn\r\n",
        "import torch.optim as optim\r\n",
        "from loss import OZELoss\r\n",
        "from transformer import Transformer\r\n",
        "import seaborn as sns\r\n",
        "from tqdm import tqdm\r\n",
        "import datetime\r\n",
        "from utils_ import compute_loss\r\n",
        "#from plot_functions import map_plot_function, plot_values_distribution, plot_error_distribution, plot_errors_threshold, plot_visual_sampl\r\n",
        "class CSVDataset(Dataset):\r\n",
        "    def __init__(self, path):\r\n",
        "        KINEMATICS_USECOLS = [c-1 for c in [39, 40, 41, 51, 52, 53, 57, 58, 59, 60, 70, 71, 72, 76]]\r\n",
        "        train = read_csv('x.txt',sep=',',usecols=KINEMATICS_USECOLS)\r\n",
        "        test = read_csv('_y.txt')\r\n",
        "        #self.X, self.y = df.values[:, :-1], df.values[:, -1]\r\n",
        "        self.X, self.y = train.values, test.values\r\n",
        "        print(self.X.shape)\r\n",
        "        self.X = self.X.astype('float32')\r\n",
        "        self.y =  LabelEncoder().fit_transform(self.y)\r\n",
        "        self.X= torch.Tensor(self.X)\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.X)\r\n",
        "    def __getitem__(self, idx):\r\n",
        "    #  if torch.is_tensor(idx):\r\n",
        "     #  idx = idx.tolist()\r\n",
        "      return [self.X[idx], self.y[idx]]\r\n",
        "    def get_splits(self, n_test=0.33):\r\n",
        "        test_size = round(n_test * len(self.X))\r\n",
        "        train_size = len(self.X) - test_size\r\n",
        "        return random_split(self, [train_size, test_size])\r\n",
        "def prepare_data(path):\r\n",
        "    dataset = CSVDataset(path)\r\n",
        "    train, val, test = dataset.get_splits()\r\n",
        "    train_dl ,val_dl,test_dl = DataLoader(train, batch_size=BATCH_SIZE,shuffle=True, num_workers=NUM_WORKERS,pin_memory=False),DataLoader(val, batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_WORKERS),DataLoader(test, batch_size=BATCH_SIZE,shuffle=False, num_workers=NUM_WORKERS)\r\n",
        "    return train_dl, val_dl, test_dl\r\n",
        "BATCH_SIZE ,NUM_WORKERS, LR,EPOCHS,d_model ,q ,v, h, N,attention_size,dropout,pe,chunk_mode,d_input,d_output= 8,0,2e-4,20,64,8 ,8,8,4,16 ,0.2,None,None,14,10\r\n",
        "path = 'train.txt'\r\n",
        "train_dl, val_dl, test_dl = prepare_data(path)\r\n",
        "print(len(train_dl.dataset), len(val_dl.dataset),  len(test_dl.dataset))\r\n",
        "sns.set()\r\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(f\"Using device {device}\")\r\n",
        "net = Transformer(d_input, d_model, d_output, q, v, h, N, attention_size=attention_size, dropout=dropout, chunk_mode=chunk_mode, pe=pe).to(device)\r\n",
        "optimizer = optim.Adam(net.parameters(), lr=LR)\r\n",
        "loss_function = nn.CrossEntropyLoss() \r\n",
        "#loss_function = OZELoss(alpha=0.3)\r\n",
        "model_save_path = f'models/model_{datetime.datetime.now().strftime(\"%Y_%m_%d__%H%M%S\")}.pth'\r\n",
        "for idx_epoch in range(EPOCHS):\r\n",
        "    running_loss = 0\r\n",
        "    with tqdm(total=len(train_dl.dataset), desc=f\"[Epoch {idx_epoch+1:3d}/{EPOCHS}]\") as pbar:\r\n",
        "      for idx_batch, (x, y) in enumerate(train_dl):\r\n",
        "        optimizer.zero_grad()\r\n",
        "        #print(idx_batch)\r\n",
        "        netout = net(x.to(device))\r\n",
        "        #print(netout.size())\r\n",
        "        #print(\"SSSSSSSS\")\r\n",
        "        #print(y.size())\r\n",
        "        loss = loss_function(netout, y)\r\n",
        "        #print(\"eee\")\r\n",
        "        loss.backward()\r\n",
        "        #print(\"kkkk\")\r\n",
        "        optimizer.step()\r\n",
        "        running_loss += loss.item()\r\n",
        "        pbar.set_postfix({'loss': running_loss/(idx_batch+1)})\r\n",
        "        pbar.update(x.shape[0])\r\n",
        "\r\n",
        "# evaluate the model\r\n",
        "def evaluate_model(test_dl, model):\r\n",
        "    predictions, actuals = list(), list()\r\n",
        "    for i, (inputs, targets) in enumerate(test_dl):\r\n",
        "        # evaluate the model on the test set\r\n",
        "        yhat = model(inputs)\r\n",
        "        # retrieve numpy array\r\n",
        "        yhat = yhat.detach().numpy()\r\n",
        "        actual = targets.numpy()\r\n",
        "        # convert to class labels\r\n",
        "        yhat = argmax(yhat, axis=1)\r\n",
        "        # reshape for stacking\r\n",
        "        actual = actual.reshape((len(actual), 1))\r\n",
        "        yhat = yhat.reshape((len(yhat), 1))\r\n",
        "        # store\r\n",
        "        predictions.append(yhat)\r\n",
        "        actuals.append(actual)\r\n",
        "    predictions, actuals = vstack(predictions), vstack(actuals)\r\n",
        "    # calculate accuracy\r\n",
        "    acc = accuracy_score(actuals, predictions)\r\n",
        "    return acc\r\n",
        "\r\n",
        "acc = evaluate_model(test_dl, net)\r\n",
        "print('Accuracy: %.3f' % acc)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ee1d0c6dd106>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'B001_X.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'B001_y.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'example.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUteCCU2J63W"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "def x():\r\n",
        "  \r\n",
        "  from sklearn.preprocessing import MinMaxScaler\r\n",
        "  from scipy.stats import zscore\r\n",
        "  KINEMATICS_USECOLS = [c-1 for c in [39, 40, 41, 51, 52, 53, 57, 58, 59, 60, 70, 71, 72, 76]]\r\n",
        "  trainX = []\r\n",
        "  filenamesTrainX = ['B001_X.txt','B002_X.txt','B003_X.txt','B004_X.txt','B005_X.txt',\r\n",
        "                    'C001_X.txt','C002_X.txt','C003_X.txt','C004_X.txt','C005_X.txt',\r\n",
        "                    'D001_X.txt','D002_X.txt','D003_X.txt','D004_X.txt','D005_X.txt',\r\n",
        "                    'E001_X.txt','E002_X.txt','E003_X.txt','E004_X.txt','E005_X.txt',\r\n",
        "                    'F001_X.txt','F002_X.txt','F003_X.txt','F004_X.txt','F005_X.txt',       \r\n",
        "                    'G001_X.txt','G002_X.txt','G003_X.txt','G004_X.txt','G005_X.txt',\r\n",
        "                    'H001_X.txt','H003_X.txt','H004_X.txt','H005_X.txt']\r\n",
        "\r\n",
        "  for fname in filenamesTrainX:\r\n",
        "      trainXdata = pd.read_csv(fname,sep=',',usecols=KINEMATICS_USECOLS)\r\n",
        "      trainXdata=trainXdata.values\r\n",
        "      trainXdata=trainXdata.astype(np.float32)\r\n",
        "      trainX.extend(trainXdata)\r\n",
        "\r\n",
        "  trainX=np.array(trainX)\r\n",
        "  print(trainX.shape)\r\n",
        "  return trainX\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKNhyG98K0VF"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\r\n",
        "def y():\r\n",
        "    trainY = []\r\n",
        "    testY2 = []\r\n",
        "    filenamesTrainY = ['B001_y.txt','B002_y.txt','B003_y.txt','B004_y.txt','B005_y.txt',\r\n",
        "                    'C001_y.txt','C002_y.txt','C003_y.txt','C004_y.txt','C005_y.txt',\r\n",
        "                    'D001_y.txt','D002_y.txt','D003_y.txt','D004_y.txt','D005_y.txt',\r\n",
        "                    'E001_y.txt','E002_y.txt','E003_y.txt','E004_y.txt','E005_y.txt',\r\n",
        "                    'F001_y.txt','F002_y.txt','F003_y.txt','F004_y.txt','F005_y.txt',\r\n",
        "                    'G001_y.txt','G002_y.txt','G003_y.txt','G004_y.txt','G005_y.txt',\r\n",
        "                    'H001_y.txt','H003_y.txt','H004_y.txt','H005_y.txt'\r\n",
        "                    ]\r\n",
        "\r\n",
        "    for fname in filenamesTrainY:\r\n",
        "      KINEMATICS_USECOLS = [c-1 for c in [39, 40, 41, 51, 52, 53, 57, 58, 59, 60, 70, 71, 72, 76,77]]\r\n",
        "      testY2 = []\r\n",
        "      trainYdata = pd.read_csv(fname)\r\n",
        "      trainYdata=trainYdata.values\r\n",
        "      trainY.extend(trainYdata)\r\n",
        "      trainY=np.array(trainY)\r\n",
        "      print(trainY.shape)\r\n",
        "      return trainY\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYScixCl50k6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw_TgHuu500W"
      },
      "source": [
        ""
      ]
    }
  ]
}